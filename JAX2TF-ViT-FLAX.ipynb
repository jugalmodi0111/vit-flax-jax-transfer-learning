{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1e71fe5",
   "metadata": {},
   "source": [
    "# Flax Runtime-Only Setup and Verification\n",
    "\n",
    "This notebook trims the repo to only what's required to run Flax as a library and verifies the installation by importing modules and running small examples.\n",
    "\n",
    "Goals:\n",
    "- Keep all code under `flax/` and `flaxlib_src/` untouched (no modules removed).\n",
    "- Remove non-runtime assets (docs, tests, examples, CI, images, notebooks inside code trees, caches).\n",
    "- Install locally in editable mode and verify imports.\n",
    "- Run minimal Linen and NNX examples, plus serialization and TrainState checks.\n",
    "\n",
    "Note: We do not remove any Python modules under `flax/`. The only internal trimming we do is safe housekeeping: delete `__pycache__` and `.ipynb` files within the package, which do not affect runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06872b8",
   "metadata": {},
   "source": [
    "## What We Keep vs Remove\n",
    "\n",
    "- Keep:\n",
    "  - `flax/` (all Python runtime code; no modules trimmed)\n",
    "  - `flaxlib_src/` (Rust/C bindings used by Flax)\n",
    "  - `pyproject.toml`, `LICENSE`, `README.md` (package + legal info)\n",
    "- Remove (non-runtime):\n",
    "  - `docs/`, `docs_nnx/`, `examples/`, `images/`, `benchmarks/`, `tests/`, `.github/`\n",
    "  - Any `.ipynb` inside `flax/` (developer notes) and `__pycache__/`\n",
    "\n",
    "> Rationale: These removed items are for documentation, testing, examples, CI, or development. They are not needed to import or run Flax in your environment. Keeping the entire `flax/` tree ensures no functionality is lost. The only internal cleanup is deleting caches and notebooks which do not affect runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c59ae90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Applications/CODES/DL - Tensorflow/flax\n",
      "Will consider removing:\n",
      "(No non-essential folders found; repo may already be trimmed.)\n"
     ]
    }
   ],
   "source": [
    "# Safety Check: Locate Non-Essential Folders\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(\"/Applications/CODES/DL - Tensorflow/flax\").resolve()\n",
    "print(\"Project root:\", ROOT)\n",
    "\n",
    "to_remove = [\n",
    "    ROOT/\"docs\",\n",
    "    ROOT/\"docs_nnx\",\n",
    "    ROOT/\"examples\",\n",
    "    ROOT/\"images\",\n",
    "    ROOT/\"benchmarks\",\n",
    "    ROOT/\"tests\",\n",
    "    ROOT/\".github\",\n",
    "]\n",
    "\n",
    "existing = [p for p in to_remove if p.exists()]\n",
    "print(\"Will consider removing:\")\n",
    "for p in existing:\n",
    "    print(\" -\", p)\n",
    "if not existing:\n",
    "    print(\"(No non-essential folders found; repo may already be trimmed.)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ba60ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# Optional Cleanup: Remove Non-Essential Folders Safely\n",
    "import shutil\n",
    "\n",
    "DO_REMOVE = False  # set to True to actually delete\n",
    "for p in to_remove:\n",
    "    if p.exists():\n",
    "        if DO_REMOVE:\n",
    "            print(\"Removing:\", p)\n",
    "            shutil.rmtree(p)\n",
    "        else:\n",
    "            print(\"Would remove:\", p)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be7d11f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 notebooks and 2 cache dirs under flax/.\n",
      "Would remove notebook: /Applications/CODES/DL - Tensorflow/flax/flax/core/flax_functional_engine.ipynb\n",
      "Would remove cache dir: /Applications/CODES/DL - Tensorflow/flax/flax/__pycache__\n",
      "Would remove cache dir: /Applications/CODES/DL - Tensorflow/flax/flax/core/__pycache__\n",
      "Housekeeping dry-run only; set DO_INTERNAL_CLEAN=True to apply.\n"
     ]
    }
   ],
   "source": [
    "# Housekeeping inside `flax/`: remove caches + notebooks (no modules removed)\n",
    "from pathlib import Path\n",
    "\n",
    "PKG = ROOT/\"flax\"\n",
    "ipynbs = list(PKG.rglob(\"*.ipynb\"))\n",
    "caches = list(PKG.rglob(\"__pycache__\"))\n",
    "print(f\"Found {len(ipynbs)} notebooks and {len(caches)} cache dirs under flax/.\")\n",
    "for p in ipynbs:\n",
    "    print(\"Would remove notebook:\", p)\n",
    "for p in caches:\n",
    "    print(\"Would remove cache dir:\", p)\n",
    "\n",
    "DO_INTERNAL_CLEAN = False  # set True to apply housekeeping\n",
    "if DO_INTERNAL_CLEAN:\n",
    "    for p in ipynbs:\n",
    "        p.unlink()\n",
    "    import shutil\n",
    "    for p in caches:\n",
    "        shutil.rmtree(p)\n",
    "    print(\"Housekeeping done.\")\n",
    "else:\n",
    "    print(\"Housekeeping dry-run only; set DO_INTERNAL_CLEAN=True to apply.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13619a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using project root: /Applications/CODES/DL - Tensorflow/flax\n",
      "/opt/anaconda3/bin/python\n",
      "$ /opt/anaconda3/bin/python -m pip install --upgrade pip\n",
      "Requirement already satisfied: pip in /opt/anaconda3/lib/python3.12/site-packages (25.3)\n",
      "\n",
      "$ /opt/anaconda3/bin/python -m pip install --upgrade jax jaxlib numpy optax\n",
      "Requirement already satisfied: pip in /opt/anaconda3/lib/python3.12/site-packages (25.3)\n",
      "\n",
      "$ /opt/anaconda3/bin/python -m pip install --upgrade jax jaxlib numpy optax\n",
      "Requirement already satisfied: jax in /opt/anaconda3/lib/python3.12/site-packages (0.8.0)\n",
      "Requirement already satisfied: jaxlib in /opt/anaconda3/lib/python3.12/site-packages (0.8.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (2.2.6)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.3.4-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting optax\n",
      "  Using cached optax-0.2.6-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from jax) (0.5.1)\n",
      "Requirement already satisfied: opt_einsum in /opt/anaconda3/lib/python3.12/site-packages (from jax) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.13 in /opt/anaconda3/lib/python3.12/site-packages (from jax) (1.13.1)\n",
      "Requirement already satisfied: absl-py>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from optax) (2.2.1)\n",
      "Collecting chex>=0.1.87 (from optax)\n",
      "  Using cached chex-0.1.91-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting absl-py>=0.7.1 (from optax)\n",
      "  Using cached absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.15.0 in /opt/anaconda3/lib/python3.12/site-packages (from chex>=0.1.87->optax) (4.15.0)\n",
      "Collecting toolz>=1.0.0 (from chex>=0.1.87->optax)\n",
      "  Using cached toolz-1.1.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Using cached optax-0.2.6-py3-none-any.whl (367 kB)\n",
      "Using cached chex-0.1.91-py3-none-any.whl (100 kB)\n",
      "Using cached absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Using cached toolz-1.1.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: toolz, absl-py, chex, optax\n",
      "\u001b[?25l\n",
      "\u001b[2K  Attempting uninstall: toolz\n",
      "\n",
      "\u001b[2K    Found existing installation: toolz 0.12.0\n",
      "\n",
      "\u001b[2K    Uninstalling toolz-0.12.0:\n",
      "\n",
      "\u001b[2K      Successfully uninstalled toolz-0.12.0\n",
      "\n",
      "\u001b[2K  Attempting uninstall: absl-py\n",
      "\n",
      "\u001b[2K    Found existing installation: absl-py 2.2.1\n",
      "\n",
      "\u001b[2K    Uninstalling absl-py-2.2.1:\n",
      "\n",
      "\u001b[2K      Successfully uninstalled absl-py-2.2.1\n",
      "\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [optax]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [optax]\n",
      "\u001b[?25h\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed absl-py-2.3.1 chex-0.1.91 optax-0.2.6 toolz-1.1.0\n",
      "\n",
      "$ /opt/anaconda3/bin/python -m pip install -e '/Applications/CODES/DL - Tensorflow/flax'\n",
      "Obtaining file:///Applications/CODES/DL%20-%20Tensorflow/flax\n",
      "\u001b[31mERROR: file:///Applications/CODES/DL%20-%20Tensorflow/flax does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "Requirement already satisfied: jax in /opt/anaconda3/lib/python3.12/site-packages (0.8.0)\n",
      "Requirement already satisfied: jaxlib in /opt/anaconda3/lib/python3.12/site-packages (0.8.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (2.2.6)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.3.4-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting optax\n",
      "  Using cached optax-0.2.6-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from jax) (0.5.1)\n",
      "Requirement already satisfied: opt_einsum in /opt/anaconda3/lib/python3.12/site-packages (from jax) (3.4.0)\n",
      "Requirement already satisfied: scipy>=1.13 in /opt/anaconda3/lib/python3.12/site-packages (from jax) (1.13.1)\n",
      "Requirement already satisfied: absl-py>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from optax) (2.2.1)\n",
      "Collecting chex>=0.1.87 (from optax)\n",
      "  Using cached chex-0.1.91-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting absl-py>=0.7.1 (from optax)\n",
      "  Using cached absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.15.0 in /opt/anaconda3/lib/python3.12/site-packages (from chex>=0.1.87->optax) (4.15.0)\n",
      "Collecting toolz>=1.0.0 (from chex>=0.1.87->optax)\n",
      "  Using cached toolz-1.1.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Using cached optax-0.2.6-py3-none-any.whl (367 kB)\n",
      "Using cached chex-0.1.91-py3-none-any.whl (100 kB)\n",
      "Using cached absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Using cached toolz-1.1.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: toolz, absl-py, chex, optax\n",
      "\u001b[?25l\n",
      "\u001b[2K  Attempting uninstall: toolz\n",
      "\n",
      "\u001b[2K    Found existing installation: toolz 0.12.0\n",
      "\n",
      "\u001b[2K    Uninstalling toolz-0.12.0:\n",
      "\n",
      "\u001b[2K      Successfully uninstalled toolz-0.12.0\n",
      "\n",
      "\u001b[2K  Attempting uninstall: absl-py\n",
      "\n",
      "\u001b[2K    Found existing installation: absl-py 2.2.1\n",
      "\n",
      "\u001b[2K    Uninstalling absl-py-2.2.1:\n",
      "\n",
      "\u001b[2K      Successfully uninstalled absl-py-2.2.1\n",
      "\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m3/4\u001b[0m [optax]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [optax]\n",
      "\u001b[?25h\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed absl-py-2.3.1 chex-0.1.91 optax-0.2.6 toolz-1.1.0\n",
      "\n",
      "$ /opt/anaconda3/bin/python -m pip install -e '/Applications/CODES/DL - Tensorflow/flax'\n",
      "Obtaining file:///Applications/CODES/DL%20-%20Tensorflow/flax\n",
      "\u001b[31mERROR: file:///Applications/CODES/DL%20-%20Tensorflow/flax does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Command failed: /opt/anaconda3/bin/python -m pip install -e '/Applications/CODES/DL - Tensorflow/flax'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDependency install warning:\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Editable install of this project\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m run(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msys\u001b[38;5;241m.\u001b[39mexecutable\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -m pip install -e \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshlex\u001b[38;5;241m.\u001b[39mquote(\u001b[38;5;28mstr\u001b[39m(ROOT))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstallation step completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(cmd)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(proc\u001b[38;5;241m.\u001b[39mstdout)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m proc\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCommand failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcmd\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Command failed: /opt/anaconda3/bin/python -m pip install -e '/Applications/CODES/DL - Tensorflow/flax'"
     ]
    }
   ],
   "source": [
    "# Environment Setup: install this package in editable mode (with fallback)\n",
    "import sys, os, subprocess, shlex, importlib\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(\"/Applications/CODES/DL - Tensorflow/flax\").resolve()\n",
    "print(\"Using project root:\", ROOT)\n",
    "\n",
    "PACKAGING_FILES = [ROOT / \"pyproject.toml\", ROOT / \"setup.py\", ROOT / \"setup.cfg\"]\n",
    "\n",
    "\n",
    "def run(cmd):\n",
    "    print(\"$\", cmd)\n",
    "    proc = subprocess.run(shlex.split(cmd), stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "    print(proc.stdout)\n",
    "    return proc.returncode\n",
    "\n",
    "print(\"Python executable:\", sys.executable)\n",
    "\n",
    "# Install common runtime deps used later in this notebook\n",
    "try:\n",
    "    run(f\"{sys.executable} -m pip install --upgrade pip\")\n",
    "    # Include TF/TFDS so later dataset and TF Hub sections work out-of-the-box\n",
    "    run(f\"{sys.executable} -m pip install --upgrade jax jaxlib numpy optax tensorflow tensorflow-datasets\")\n",
    "except Exception as e:\n",
    "    print(\"Dependency install warning:\", e)\n",
    "\n",
    "installed = False\n",
    "if any(p.exists() for p in PACKAGING_FILES):\n",
    "    rc = run(f\"{sys.executable} -m pip install -e {shlex.quote(str(ROOT))}\")\n",
    "    if rc == 0:\n",
    "        installed = True\n",
    "        print(\"Editable install completed.\")\n",
    "    else:\n",
    "        print(\"Editable install failed; falling back to source path import.\")\n",
    "else:\n",
    "    print(\"No packaging metadata found in project root (no pyproject/setup). Using source path import.\")\n",
    "\n",
    "# Fallback: import directly from source tree by putting project root on sys.path\n",
    "if not installed:\n",
    "    if str(ROOT) not in sys.path:\n",
    "        sys.path.insert(0, str(ROOT))\n",
    "    try:\n",
    "        import flax as _flax_check\n",
    "        print(\"Imported flax from source path:\", Path(_flax_check.__file__).resolve())\n",
    "    except Exception as e:\n",
    "        print(\"Source-path import of flax failed:\", type(e).__name__, str(e)[:300])\n",
    "        raise\n",
    "\n",
    "# Final sanity print\n",
    "try:\n",
    "    import flax\n",
    "    ver = getattr(flax, \"__version__\", \"(no __version__)\")\n",
    "    print(\"flax import OK; version:\", ver)\n",
    "except Exception as e:\n",
    "    print(\"flax import failed even after fallback:\", type(e).__name__, str(e)[:300])\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb186300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Verification: core modules\n",
    "import importlib, pkgutil, traceback\n",
    "mods = [\n",
    "    \"flax\",\n",
    "    \"flax.core\",\n",
    "    \"flax.linen\",\n",
    "    \"flax.training\",\n",
    "    \"flax.serialization\",\n",
    "    \"flax.traverse_util\",\n",
    "    \"flax.struct\",\n",
    "    \"flax.jax_utils\",\n",
    "    # Experimental / optional:\n",
    "    \"flax.nnx\",\n",
    "    # Avoid importing tensorboard utilities by default (optional deps).\n",
    "]\n",
    "results = {}\n",
    "for m in mods:\n",
    "    try:\n",
    "        importlib.import_module(m)\n",
    "        results[m] = \"OK\"\n",
    "    except Exception as e:\n",
    "        results[m] = f\"FAIL: {e.__class__.__name__}: {e}\"\n",
    "for k,v in results.items():\n",
    "    print(f\"{k:25s} -> {v}\")\n",
    "\n",
    "# Optionally attempt to discover and import subpackages under flax/ (best-effort)\n",
    "print(\"\\nBest-effort import of subpackages (skips known optional/deps-heavy areas)...\")\n",
    "import flax, types\n",
    "skips = {\"metrics\", \"testing\"}\n",
    "for f in pkgutil.iter_modules(flax.__path__, prefix=\"flax.\"):\n",
    "    name = f.name.split(\".\")[-1]\n",
    "    if name in skips:\n",
    "        print(f\"skip {f.name}\")\n",
    "        continue\n",
    "    try:\n",
    "        importlib.import_module(f.name)\n",
    "        print(\"ok  \", f.name)\n",
    "    except Exception as e:\n",
    "        print(\"fail\", f.name, \"->\", e.__class__.__name__, str(e)[:140])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f55a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linen Example: simple MLP forward pass\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    features: tuple\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        for feat in self.features[:-1]:\n",
    "            x = nn.Dense(feat)(x)\n",
    "            x = nn.relu(x)\n",
    "        x = nn.Dense(self.features[-1])(x)\n",
    "        return x\n",
    "\n",
    "key = jax.random.key(0)\n",
    "x = jax.random.normal(key, (4, 8))\n",
    "model = MLP(features=(16, 4))\n",
    "params = model.init(key, x)\n",
    "y = model.apply(params, x)\n",
    "print(\"Output shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b416399e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialization roundtrip\n",
    "from flax import serialization\n",
    "\n",
    "state_dict = serialization.to_state_dict(params)\n",
    "restored = serialization.from_state_dict(params, state_dict)\n",
    "# sanity: compare a leaf\n",
    "import jax.tree_util as jtu\n",
    "leaves_a, _ = jtu.tree_flatten(params)\n",
    "leaves_b, _ = jtu.tree_flatten(restored)\n",
    "print(\"num leaves:\", len(leaves_a))\n",
    "print(\"allclose:\", all(jnp.allclose(a, b) for a,b in zip(leaves_a, leaves_b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af45708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TrainState example with Optax\n",
    "import optax\n",
    "from flax.training import train_state\n",
    "\n",
    "def loss_fn(params, x):\n",
    "    y = model.apply(params, x)\n",
    "    return jnp.mean(jnp.square(y))\n",
    "\n",
    "tx = optax.adam(1e-2)\n",
    "state = train_state.TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state, x):\n",
    "    l, grads = jax.value_and_grad(loss_fn)(state.params, x)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, l\n",
    "\n",
    "for i in range(3):\n",
    "    state, l = train_step(state, x)\n",
    "    print(f\"step {i}: loss={float(l):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b718b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NNX Example (best-effort; optional depending on version)\n",
    "try:\n",
    "    import flax.nnx as nnx\n",
    "    class NNXMLP(nnx.Module):\n",
    "        def __init__(self, in_features, hidden_features, out_features, *, rngs: nnx.Rngs):\n",
    "            self.d1 = nnx.Linear(in_features, hidden_features, rngs=rngs)\n",
    "            self.d2 = nnx.Linear(hidden_features, out_features, rngs=rngs)\n",
    "        def __call__(self, x):\n",
    "            return self.d2(nnx.relu(self.d1(x)))\n",
    "    rngs = nnx.Rngs(0)\n",
    "    model_nnx = NNXMLP(8, 16, 4, rngs=rngs)\n",
    "    y2 = model_nnx(x)\n",
    "    print(\"NNX forward ok, shape:\", y2.shape)\n",
    "except Exception as e:\n",
    "    print(\"NNX example skipped:\", e.__class__.__name__, str(e)[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a143f0b4",
   "metadata": {},
   "source": [
    "## Trimming Justification\n",
    "- `docs/`, `docs_nnx/`: documentation sources; not needed to import or run Flax.\n",
    "- `examples/`, `benchmarks/`: usage samples and perf scripts; not required at runtime.\n",
    "- `tests/`: test suite only; not required by users of the library.\n",
    "- `images/`: static assets for docs; safe to remove for runtime.\n",
    "- `.github/`: CI and templates; no effect on local import or execution.\n",
    "- Notebooks inside `flax/`: developer materials; safe to remove for runtime.\n",
    "- `__pycache__/`: compiled bytecode caches; recreated on demand and safe to delete.\n",
    "\n",
    "We deliberately keep the entire `flax/` package tree and `flaxlib_src/` to ensure no functional module is omitted. If any optional-import failure occurs (e.g., tensorboard/Orbax), install those extras only when you use those features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fd8d48",
   "metadata": {},
   "source": [
    "## Full Correlation & Justification Report\n",
    "This section scans the project and labels every file and folder as either `keep` or `remove`, with a one-line justification for each decision. Nothing is left unclassified — the report covers all paths under `flax/` and `flaxlib_src/` so you can review before applying any deletions.\n",
    "\n",
    "Policy recap:\n",
    "- Keep: all runtime code under `flax/` (the Python package) and `flaxlib_src/` (native bindings), plus packaging files within the `flax/` project folder (e.g., `pyproject.toml`, `setup.cfg`, `setup.py`) if present.\n",
    "- Remove: non-runtime collateral under the `flax/` project folder such as `docs/`, `docs_nnx/`, `examples/`, `images/`, `benchmarks/`, `tests/`, `.github/`, developer notebooks inside `flax/flax/`, and any `__pycache__/` directories.\n",
    "\n",
    "Outputs:\n",
    "- `trim_report.json`: exhaustive per-path decisions with reasons.\n",
    "- `trim_report.csv`: CSV version for quick scanning.\n",
    "- Optional: an apply step (next cell) that enforces removals with a safety toggle and writes `trim_log.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78fbee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlate all paths and generate report\n",
    "from pathlib import Path\n",
    "import json, csv\n",
    "from typing import List, Dict\n",
    "\n",
    "WS = Path(\"/Applications/CODES/DL - Tensorflow\").resolve()\n",
    "FLAX_PROJECT = WS / \"flax\"\n",
    "FLAX_PKG = FLAX_PROJECT / \"flax\"  # python package root\n",
    "FLAXLIB_SRC = WS / \"flaxlib_src\"\n",
    "\n",
    "print(\"Workspace:\", WS)\n",
    "print(\"Flax project root:\", FLAX_PROJECT)\n",
    "print(\"Flax package root:\", FLAX_PKG)\n",
    "print(\"flaxlib_src root:\", FLAXLIB_SRC)\n",
    "\n",
    "targets: List[Path] = [p for p in [FLAX_PROJECT, FLAXLIB_SRC] if p.exists()]\n",
    "if not targets:\n",
    "    raise SystemExit(\"No targets found to scan (expected 'flax/' or 'flaxlib_src/').\")\n",
    "\n",
    "REMOVE_DIR_NAMES = {\"docs\", \"docs_nnx\", \"examples\", \"images\", \"benchmarks\", \"tests\", \".github\"}\n",
    "\n",
    "def under(path: Path, base: Path) -> bool:\n",
    "    try:\n",
    "        path.resolve().relative_to(base.resolve())\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def decide(p: Path):\n",
    "    rel = p.resolve().relative_to(WS)\n",
    "    is_dir = p.is_dir()\n",
    "    # Default\n",
    "    action, reason = \"keep\", \"out of scope\"\n",
    "\n",
    "    # Always keep the control notebook\n",
    "    if rel.as_posix() == \"JAX2TF-ViT-FLAX.ipynb\":\n",
    "        return action, \"control notebook (keep)\"\n",
    "\n",
    "    # Cache directories\n",
    "    if is_dir and p.name == \"__pycache__\":\n",
    "        return \"remove\", \"bytecode cache (safe to regenerate)\"\n",
    "\n",
    "    # Non-runtime collateral inside the Flax project\n",
    "    if under(p, FLAX_PROJECT):\n",
    "        # Remove well-known non-runtime directories anywhere under project\n",
    "        if any(name in p.parts for name in REMOVE_DIR_NAMES):\n",
    "            return \"remove\", \"non-runtime collateral (docs/tests/examples/ci/assets)\"\n",
    "        # Developer notebooks inside the python package\n",
    "        if under(p, FLAX_PKG) and p.suffix == \".ipynb\":\n",
    "            return \"remove\", \"developer notebook inside package (not required at runtime)\"\n",
    "        # Packaging files in project root are kept\n",
    "        if p.parent == FLAX_PROJECT and p.name in {\"pyproject.toml\", \"setup.cfg\", \"setup.py\", \"README.md\", \"LICENSE\"}:\n",
    "            return \"keep\", \"project packaging/metadata (required to build/install)\"\n",
    "        # Keep everything else under the python package\n",
    "        if under(p, FLAX_PKG):\n",
    "            return \"keep\", \"Flax runtime package\"\n",
    "        # Otherwise, default keep within project unless matched above\n",
    "        return \"keep\", \"project runtime/ancillary needed for install\"\n",
    "\n",
    "    # Keep native sources entirely\n",
    "    if under(p, FLAXLIB_SRC):\n",
    "        return \"keep\", \"native bindings/runtime sources\"\n",
    "\n",
    "    return action, reason\n",
    "\n",
    "# Collect all paths (including the roots)\n",
    "all_paths: List[Path] = []\n",
    "for base in targets:\n",
    "    all_paths.append(base)\n",
    "    all_paths.extend(sorted(base.rglob(\"*\")))\n",
    "\n",
    "decisions: List[Dict] = []\n",
    "for p in all_paths:\n",
    "    act, why = decide(p)\n",
    "    decisions.append({\n",
    "        \"path\": str(p.resolve().relative_to(WS)),\n",
    "        \"type\": \"dir\" if p.is_dir() else \"file\",\n",
    "        \"action\": act,\n",
    "        \"reason\": why,\n",
    "    })\n",
    "\n",
    "keep_n = sum(1 for d in decisions if d[\"action\"] == \"keep\")\n",
    "rem_n = sum(1 for d in decisions if d[\"action\"] == \"remove\")\n",
    "print(f\"Decisions: keep={keep_n}, remove={rem_n}, total={len(decisions)}\")\n",
    "\n",
    "# Write reports\n",
    "report_json = WS / \"trim_report.json\"\n",
    "report_csv = WS / \"trim_report.csv\"\n",
    "with report_json.open(\"w\") as f:\n",
    "    json.dump(decisions, f, indent=2)\n",
    "with report_csv.open(\"w\", newline=\"\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=[\"path\",\"type\",\"action\",\"reason\"])\n",
    "    w.writeheader()\n",
    "    w.writerows(decisions)\n",
    "print(\"Wrote:\", report_json)\n",
    "print(\"Wrote:\", report_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437f20d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply removals from report (guarded)\n",
    "import json, shutil, os\n",
    "from pathlib import Path\n",
    "\n",
    "WS = Path(\"/Applications/CODES/DL - Tensorflow\").resolve()\n",
    "FLAX_PROJECT = WS / \"flax\"\n",
    "FLAX_PKG = FLAX_PROJECT / \"flax\"\n",
    "FLAXLIB_SRC = WS / \"flaxlib_src\"\n",
    "\n",
    "report_json = WS / \"trim_report.json\"\n",
    "if not report_json.exists():\n",
    "    raise SystemExit(\"trim_report.json not found. Run the previous cell first.\")\n",
    "\n",
    "with report_json.open() as f:\n",
    "    decisions = json.load(f)\n",
    "\n",
    "def safe_under(p: Path) -> bool:\n",
    "    # Only allow deletions under the intended project trees\n",
    "    return any(str(p).startswith(str(base)) for base in [FLAX_PROJECT, FLAXLIB_SRC])\n",
    "\n",
    "APPLY_REMOVE = False  # set True to actually delete\n",
    "removed, failed = 0, 0\n",
    "log_lines = []\n",
    "\n",
    "# Delete deepest paths first to handle directories after files\n",
    "for d in sorted((x for x in decisions if x[\"action\"] == \"remove\"), key=lambda x: x[\"path\"].count(\"/\"), reverse=True):\n",
    "    p = (WS / d[\"path\"]).resolve()\n",
    "    if not safe_under(p):\n",
    "        log_lines.append(f\"SKIP (unsafe): {p}\")\n",
    "        continue\n",
    "    if APPLY_REMOVE:\n",
    "        try:\n",
    "            if p.is_dir():\n",
    "                shutil.rmtree(p)\n",
    "            elif p.exists():\n",
    "                p.unlink()\n",
    "            log_lines.append(f\"REMOVED: {p}\")\n",
    "            removed += 1\n",
    "        except Exception as e:\n",
    "            log_lines.append(f\"FAILED: {p} -> {e}\")\n",
    "            failed += 1\n",
    "    else:\n",
    "        log_lines.append(f\"DRY-RUN would remove: {p}\")\n",
    "\n",
    "log_path = WS / \"trim_log.txt\"\n",
    "with log_path.open(\"w\") as f:\n",
    "    f.write(\"\\n\".join(log_lines))\n",
    "\n",
    "mode = \"APPLIED\" if APPLY_REMOVE else \"DRY-RUN\"\n",
    "print(f\"{mode}: decisions processed. To actually delete, set APPLY_REMOVE=True and re-run.\")\n",
    "print(f\"Entries marked remove: {sum(1 for d in decisions if d['action']=='remove')} | removed={removed} | failed={failed}\")\n",
    "print(\"Wrote:\", log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4457fe4a",
   "metadata": {},
   "source": [
    "## Datasets: CIFAR-10 and ImageNet-1k\n",
    "This notebook can run quick demos and fine-tunes on two canonical datasets:\n",
    "\n",
    "- CIFAR-10: 5/5 — Krizhevsky, 2009 (https://www.cs.toronto.edu/~kriz/cifar.html).\n",
    "  Usability: TFDS `cifar10` auto-downloads; tiny and ideal for sanity checks.\n",
    "- ImageNet-1k (ILSVRC 2012): 5/5 — Russakovsky et al., 2015 (https://image-net.org/challenges/LSVRC/2012/).\n",
    "  Usability: TFDS `imagenet2012` requires manual download/acceptance; large and best with GPU/TPU.\n",
    "\n",
    "ImageNet-1k manual setup (TFDS):\n",
    "- Create a data directory and set `TFDS_DATA_DIR` or pass `data_dir` to loaders.\n",
    "- Follow TFDS instructions to place tar files. See: https://www.tensorflow.org/datasets/catalog/imagenet2012#manual_download_instructions\n",
    "\n",
    "Example (zsh):\n",
    "```zsh\n",
    "# Choose a data directory\n",
    "export TFDS_DATA_DIR=\"$HOME/tfds_data\"\n",
    "mkdir -p \"$TFDS_DATA_DIR\"\n",
    "# Then run the ImageNet loader cell in this notebook; it will use TFDS.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85485aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFDS dataset utilities for CIFAR-10 and ImageNet-1k\n",
    "import os\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    import tensorflow_datasets as tfds\n",
    "except Exception as e:\n",
    "    print(\"TensorFlow/TFDS not found. Install with:\")\n",
    "    print(\"  python -m pip install -U tensorflow tensorflow-datasets\")\n",
    "    raise\n",
    "\n",
    "AUTOTUNE = getattr(tf.data, \"AUTOTUNE\", 16)\n",
    "\n",
    "def _resize_to(image, size=224, antialias=True):\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    image = tf.image.resize(image, (size, size), method=\"bilinear\", antialias=antialias)\n",
    "    return tf.clip_by_value(image, 0.0, 1.0)\n",
    "\n",
    "def _augment_train(image):\n",
    "    # Light augment: random flip; you can extend as needed\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    return image\n",
    "\n",
    "def _prep_example(example, size=224, train=False):\n",
    "    image = example[\"image\"]\n",
    "    label = tf.cast(example[\"label\"], tf.int32)\n",
    "    image = _resize_to(image, size)\n",
    "    if train:\n",
    "        image = _augment_train(image)\n",
    "    return image, label\n",
    "\n",
    "def _make_pipeline(ds, batch_size, size=224, train=False, shuffle=True):\n",
    "    if train and shuffle:\n",
    "        ds = ds.shuffle(10_000)\n",
    "    ds = ds.map(lambda ex: _prep_example(ex, size=size, train=train), num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.batch(batch_size).prefetch(AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "def get_cifar10_datasets(batch_size=64, size=224, data_dir=None):\n",
    "    ds_train, ds_val = tfds.load(\"cifar10\", split=[\"train\", \"test\"], data_dir=data_dir, as_supervised=False, with_info=False)\n",
    "    return (\n",
    "        _make_pipeline(ds_train, batch_size, size=size, train=True, shuffle=True),\n",
    "        _make_pipeline(ds_val, batch_size, size=size, train=False, shuffle=False),\n",
    "    )\n",
    "\n",
    "def get_imagenet_datasets(batch_size=128, size=224, data_dir=None):\n",
    "    name = \"imagenet2012\"\n",
    "    try:\n",
    "        builder = tfds.builder(name, data_dir=data_dir)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"TFDS builder for ImageNet-1k not available. Set TFDS_DATA_DIR or data_dir and ensure manual files are placed.\") from e\n",
    "    # Do NOT attempt download here; expects manual files already in place per TFDS docs.\n",
    "    if not builder.info.splits:\n",
    "        # Trigger metadata preparation without download if already present\n",
    "        pass\n",
    "    try:\n",
    "        ds_train = tfds.load(name, split=\"train\", data_dir=data_dir, as_supervised=False)\n",
    "        ds_val = tfds.load(name, split=\"validation\", data_dir=data_dir, as_supervised=False)\n",
    "    except Exception as e:\n",
    "        msg = [\n",
    "            \"Could not load ImageNet-1k via TFDS.\",\n",
    "            \"Ensure manual download is completed per https://www.tensorflow.org/datasets/catalog/imagenet2012#manual_download_instructions\",\n",
    "            \"Set TFDS_DATA_DIR or pass data_dir to this function.\",\n",
    "        ]\n",
    "        raise RuntimeError(\"\\n\".join(msg)) from e\n",
    "    return (\n",
    "        _make_pipeline(ds_train, batch_size, size=size, train=True, shuffle=True),\n",
    "        _make_pipeline(ds_val, batch_size, size=size, train=False, shuffle=False),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9647fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick run: CIFAR-10 demo (sanity check)\n",
    "try:\n",
    "    ds_tr, ds_val = get_cifar10_datasets(batch_size=64, size=224)\n",
    "    for images, labels in ds_tr.take(1):\n",
    "        print(\"CIFAR-10 train batch:\", images.shape, labels.shape, images.dtype, labels.dtype)\n",
    "    for images, labels in ds_val.take(1):\n",
    "        print(\"CIFAR-10 val batch:\", images.shape, labels.shape)\n",
    "except Exception as e:\n",
    "    print(\"CIFAR-10 loader error:\", type(e).__name__, str(e)[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8ea9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick run: ImageNet-1k loader check\n",
    "import os\n",
    "IMAGENET_DATA_DIR = os.getenv(\"TFDS_DATA_DIR\")  # or set to a path string\n",
    "print(\"TFDS_DATA_DIR:\", IMAGENET_DATA_DIR)\n",
    "\n",
    "try:\n",
    "    ds_tr_imnet, ds_val_imnet = get_imagenet_datasets(batch_size=128, size=224, data_dir=IMAGENET_DATA_DIR)\n",
    "    for images, labels in ds_val_imnet.take(1):\n",
    "        print(\"ImageNet val batch:\", images.shape, labels.shape, images.dtype, labels.dtype)\n",
    "except Exception as e:\n",
    "    print(\"ImageNet loader notice:\")\n",
    "    print(str(e))\n",
    "    print(\"If you have the data, set TFDS_DATA_DIR to its location and re-run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75bbfd2",
   "metadata": {},
   "source": [
    "## CIFAR-10 fine-tuning with a TF Hub ViT (explained simply)\n",
    "We will:\n",
    "1) Install the TF Hub package (lets us load the ViT).\n",
    "2) Pick a ViT feature-extractor from TF Hub (no classification head).\n",
    "3) Build a small Keras model: ViT (features) -> Dense(10) for CIFAR-10 classes.\n",
    "4) Train for a few epochs and see accuracy.\n",
    "5) Evaluate on the validation set.\n",
    "\n",
    "Why this works:\n",
    "- The TF Hub ViT gives a general image representation.\n",
    "- The small Dense layer learns to map that representation to 10 CIFAR classes.\n",
    "- This is fast and usually reaches good accuracy with very little code.\n",
    "\n",
    "Tip: You can freeze the ViT (faster) or unfreeze it (more accuracy). We start frozen, then optionally unfreeze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877a4c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Install/import TensorFlow Hub (only runs once)\n",
    "import sys, subprocess, shlex\n",
    "def _run(cmd):\n",
    "    print(\"$\", cmd)\n",
    "    p = subprocess.run(shlex.split(cmd), stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "    print(p.stdout)\n",
    "    if p.returncode != 0:\n",
    "        raise RuntimeError(f\"Command failed: {cmd}\")\n",
    "try:\n",
    "    import tensorflow_hub as hub\n",
    "except Exception:\n",
    "    _run(f\"{sys.executable} -m pip install -U tensorflow-hub\")\n",
    "    import tensorflow_hub as hub\n",
    "print(\"tensorflow-hub version:\", hub.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f43244f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Choose a TF Hub ViT feature extractor (no classification head)\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# You can change this to another handle from the same collection.\n",
    "# If loading fails, pick a different handle or version.\n",
    "HUB_HANDLES = [\n",
    "    \"https://tfhub.dev/sayakpaul/vit_b16_fe/1\",\n",
    "    \"https://tfhub.dev/sayakpaul/vit_s16_fe/1\",\n",
    "    # Add more known handles here if needed\n",
    "]\n",
    "\n",
    "selected_handle = None\n",
    "for h in HUB_HANDLES:\n",
    "    try:\n",
    "        _ = hub.KerasLayer(h, trainable=False)\n",
    "        selected_handle = h\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(\"Handle failed:\", h, \"->\", type(e).__name__, str(e)[:120])\n",
    "\n",
    "if selected_handle is None:\n",
    "    raise SystemExit(\"No TF Hub ViT handle could be loaded. Please set HUB_HANDLES[0] to a valid handle from the README.\")\n",
    "\n",
    "print(\"Using TF Hub handle:\", selected_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb28fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Build the model: ViT feature extractor -> Dense(10)\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "inputs = tf.keras.Input(shape=(224, 224, 3), name=\"image\")\n",
    "vit = hub.KerasLayer(selected_handle, trainable=False, name=\"vit_fe\")\n",
    "features = vit(inputs)  # typically a 1D embedding\n",
    "outputs = tf.keras.layers.Dense(10, activation=\"softmax\", name=\"classifier\")(features)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8be248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Train for a few epochs on CIFAR-10 (simple and quick)\n",
    "BATCH = 64\n",
    "EPOCHS = 3  # increase for better accuracy\n",
    "\n",
    "ds_tr, ds_val = get_cifar10_datasets(batch_size=BATCH, size=224)\n",
    "steps_tr = 50   # limit steps for a quicker demo; set None for full epoch\n",
    "steps_val = 20  # limit steps for a quicker demo; set None for full epoch\n",
    "\n",
    "history = model.fit(\n",
    "    ds_tr,\n",
    "    validation_data=ds_val,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=steps_tr,\n",
    "    validation_steps=steps_val,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8445f687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Evaluate and (optional) unfreeze ViT for extra accuracy\n",
    "val_loss, val_acc = model.evaluate(ds_val, steps=steps_val)\n",
    "print({\"val_loss\": float(val_loss), \"val_acc\": float(val_acc)})\n",
    "\n",
    "DO_UNFREEZE = False  # set True to fine-tune the ViT weights as well\n",
    "if DO_UNFREEZE:\n",
    "    print(\"Unfreezing the ViT and training briefly...\")\n",
    "    model.get_layer(\"vit_fe\").trainable = True\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(1e-5),  # lower LR when unfreezing\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "    history2 = model.fit(\n",
    "        ds_tr,\n",
    "        validation_data=ds_val,\n",
    "        epochs=1,\n",
    "        steps_per_epoch=steps_tr,\n",
    "        validation_steps=steps_val,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
